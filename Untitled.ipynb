{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ff36bc8-c716-4837-8b5c-d8efed911b0c",
   "metadata": {},
   "source": [
    "# Finding Experience Level of Gym Members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d2e996-75c3-4255-b345-6b65c29e9daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing Necessary Libraries and Packages.\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import make_scorer, accuracy_score,precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "from imblearn.over_sampling._smote.base import SMOTE\n",
    "import missingno as msno\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d26afc-689d-40a8-9994-dd5f30d1b025",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Importing necessary libraries and packages.\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.config.optimizer.set_jit(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46585c04-f340-401a-b074-d7f6e3a7241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualization settings\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "sns.set_style(\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b7c876-b1a2-4484-b366-f66396351f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Pandas Setting\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.width', 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89774256-e8ba-4019-884c-a1ec2f93cc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Reading Data\n",
    "\n",
    "data = pd.read_csv('/Users/erkan/Downloads/feature_engineering/feature_engineering/datasets/gym_members_exercise_tracking.csv')\n",
    "np.random.seed(12345)\n",
    "dataset = data.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e343ee6-233d-4fc7-b38b-8256737a3a56",
   "metadata": {},
   "source": [
    "## First sight at the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd314c8-d33b-44c0-832c-9034e01debb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bfdcb7-0c38-4cca-8d72-9381da3b6412",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d9090d-a411-4586-9dfa-d8afb67fb255",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset.isnull().sum()\n",
    "\n",
    "### There is no null in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecefe89-f71a-4fca-9dca-6fd9acdf0b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset.duplicated().sum()\n",
    "\n",
    "### There is no duplicated instances in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e123a977-4aa7-425e-9c9b-419238b6129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810975c6-1cf0-4b70-9640-ddfcc066987a",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6966c2-d50f-42fb-98b0-e52bd29a1844",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Grabbing categorical and numerical columns\n",
    "\n",
    "def grab_col_names(dataframe, cat_th=10, car_th=20):\n",
    "\n",
    "    # cat_cols, cat_but_car\n",
    "    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n",
    "    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n",
    "                   dataframe[col].dtypes != \"O\"]\n",
    "    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n",
    "                   dataframe[col].dtypes == \"O\"]\n",
    "    cat_cols = cat_cols + num_but_cat\n",
    "    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n",
    "\n",
    "    # num_cols\n",
    "    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n",
    "    num_cols = [col for col in num_cols if col not in num_but_cat]\n",
    "\n",
    "    print(f\"Observations: {dataframe.shape[0]}\")\n",
    "    print(f\"Variables: {dataframe.shape[1]}\")\n",
    "    print(f'cat_cols: {len(cat_cols)}')\n",
    "    print(f'num_cols: {len(num_cols)}')\n",
    "    print(f'cat_but_car: {len(cat_but_car)}')\n",
    "    print(f'num_but_cat: {len(num_but_cat)}')\n",
    "    return cat_cols, num_cols, cat_but_car\n",
    "\n",
    "\n",
    "cat_cols, num_cols, cat_but_car = grab_col_names(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d2c2f1-1462-4618-ba23-c32c7694ad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Numerical columns analysis\n",
    "\n",
    "dataset.describe().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86712cab-293d-4a17-87a2-8e371d7c494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Distrubition Of Numerical Columns.\n",
    "\n",
    "for col in num_cols:\n",
    "    sns.histplot(x=col, data=dataset)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7277737b-91e2-441d-9ee7-c3bb137e8e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Outlier Detection of Numerical Columns with Boxplot\n",
    "\n",
    "for col in num_cols:\n",
    "    sns.boxplot(x=col, data=dataset)\n",
    "    plt.show()\n",
    "\n",
    "# BMI, Calories Burned, Weight columns have outliers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddfb009-7955-47f8-9b50-df4f28d16023",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Relation Between Numerical Column And Target\n",
    "\n",
    "for col in num_cols:\n",
    "    sns.scatterplot(x=col, data=dataset, y= 'Experience_Level')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f0bcfd-1694-4c77-bd6f-889caa58e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Looking the relationship between target columns and numerical columns.\n",
    "\n",
    "def target_summary_with_num(dataframe, target, numerical_col):\n",
    "    print(dataframe.groupby(target).agg({numerical_col: \"mean\"}), end=\"\\n\\n\")\n",
    "\n",
    "for col in num_cols:\n",
    "    target_summary_with_num(dataset, \"Experience_Level\", col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34f50f4-5572-42dd-876b-a0c83705be01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Correlation Analysis\n",
    "\n",
    "cor_matrix = dataset[num_cols].corr()\n",
    "\n",
    "cor_matrix\n",
    "\n",
    "sns.set(rc={'figure.figsize': (12, 12)})\n",
    "sns.heatmap(cor_matrix, cmap=\"RdBu\")\n",
    "plt.show()\n",
    "\n",
    "# There is negative corr between the fat percentage and the both Sessin Duration and Calories burned\n",
    "# and there is positive corr between BMI and Weight as expected.\n",
    "# Absolute values of other corrs is less than 0.5 which is not necessary to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d556ccaa-0ab2-4c80-b557-b94dcd6cb5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Categorical Columns Analysis\n",
    "\n",
    "\n",
    "def cat_summary(dataframe, col_name):\n",
    "    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),\n",
    "                        \"Ratio\": 100 * dataframe[col_name].value_counts() / len(dataframe)}))\n",
    "    print(\"##########################################\")\n",
    "\n",
    "for col in cat_cols:\n",
    "    cat_summary(dataset, col)\n",
    "\n",
    "for col in cat_cols:\n",
    "    sns.countplot(x=dataset[col], data=dataset)\n",
    "    plt.show(block = True)\n",
    "\n",
    "\n",
    "# Dataset is imbalance, some techniques could be used;\n",
    "# in order to create more robust model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64e1bb8-3018-4b30-9a6d-ce348ef6a7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### Relation Between Categorical Columns And Target\n",
    "\n",
    "def target_summary_with_cat(dataframe, target, categorical_col):\n",
    "    summary_df = dataframe.groupby(categorical_col).agg(\n",
    "        TARGET_MEAN=(target, 'mean'),\n",
    "        COUNTS=(target, 'count')\n",
    "    ).reset_index()\n",
    "\n",
    "    print(summary_df, end=\"\\n\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292ed5ac-848c-49fe-b100-00a4144e46ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e500c09f-b6a8-4a7a-9175-0813081b747c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49820a82-04c1-42d2-aa05-742b62460738",
   "metadata": {},
   "source": [
    "## Feature Engineering with Columntransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2861bb-68a2-4466-b3ce-85df9872d248",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Outliers\n",
    "\n",
    "# Since we will use models that sensitive to outliers, they should be considered.\n",
    "# From EDA part, we know; BMI, Calories Burned, Weight columns has outliers.\n",
    "\n",
    "has_out = ['BMI','Calories_Burned',  'Weight (kg)']\n",
    "\n",
    "#BMI\n",
    "\n",
    "dataset['BMI'].describe()\n",
    "dataset.groupby('Experience_Level').agg({'BMI' : 'mean'})\n",
    "\n",
    "#### Solutions for IBM\n",
    "\n",
    "## I dont want to use iqr to replace outliers with quartiles because;\n",
    "# we have also height and weight which should also be considered.\n",
    "\n",
    "## Since ıbm can be categorized and that can be solve our outlier problem but;\n",
    "# we have to consider curse of dimensionality.\n",
    "# We can get rid of curse of dimensionality by using categorical mapping, instead of one hot encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f0b91b-2cf3-4a62-ba71-ace37dbabbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorizing\n",
    "\n",
    "bins = [0, 18.5, 24.9, 29.9, 34.9, 39.9, float('inf')]\n",
    "labels = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# Use pd.cut() to categorize the BMI values into the defined bins.\n",
    "\n",
    "dataset['BMI_Label'] = pd.cut(dataset['BMI'], bins=bins, labels=labels, right=False)\n",
    "dataset.drop(columns = 'BMI', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2a880a-eef9-49bf-a673-63632c27cb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Calories_Burned'].describe()\n",
    "\n",
    "# Robust Scaling could be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436fd598-a6b8-458a-9f49-eb6aa3fc6d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset['Weight (kg)'].describe()\n",
    "\n",
    "# Robust Scaling could be used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d0a4a8-1f5e-4ce8-b2ac-25127653d831",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting data in to x train/test and y train/test\n",
    "\n",
    "X = dataset.drop(columns=['Experience_Level'])  \n",
    "y = dataset['Experience_Level']       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07861cbe-c1f7-45eb-8896-f65fa4995eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eed46bf-f249-4a8d-99bb-c8cf298b0772",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATİNG COLUMNTRANSFORMER FOR PREPROCESSİNG\n",
    "\n",
    "\n",
    "std = ['Height (m)', 'Max_BPM', 'Avg_BPM', 'Resting_BPM', 'Session_Duration (hours)' , 'Calories_Burned','Fat_Percentage', 'Water_Intake (liters)', 'Workout_Frequency (days/week)']\n",
    "rbst = ['Calories_Burned', 'Weight (kg)']\n",
    "ohe = ['Workout_Type' , 'Gender']\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "        ('ohe', OneHotEncoder(drop = 'first'), ohe),\n",
    "        ('std_scaler', StandardScaler(), std),\n",
    "        ('rbst_scaler', RobustScaler(), rbst)  \n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1471eae3-d6cb-485a-a224-fe432d4233c2",
   "metadata": {},
   "source": [
    "## Building KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b020b808-6861-41bd-8192-d28ec61ca88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### CROSS VALİDATİON FOR DETERMİNİNG K FOR KNN\n",
    "\n",
    "\n",
    "k_values = [3, 5, 7, 9, 11, 13, 15]\n",
    "\n",
    "sf = StratifiedKFold(n_splits=5, shuffle=True, random_state=12345)\n",
    "\n",
    "mean_scores = {}\n",
    "\n",
    "for k in k_values:\n",
    "    pipeline.set_params(classifier__n_neighbors=k)\n",
    "    scores = cross_val_score(pipeline, X, y, cv=sf, scoring='accuracy')\n",
    "    mean_scores[k] = scores.mean()\n",
    "\n",
    "best_k = max(mean_scores, key=mean_scores.get)\n",
    "\n",
    "print(f\"Best k value: {best_k}\")\n",
    "print(f\"Best cross-validation accuracy: {mean_scores[best_k]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17617676-ece3-4c8b-9230-3dd274a7a830",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## EVALUTAION FOR K=15\n",
    "\n",
    "pipeline = imbpipeline([ \n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE()),\n",
    "    ('classifier', KNeighborsClassifier(n_neighbors=15))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "###  Evaluation metrics\n",
    "\n",
    "# Accuracy \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# ROC AUC\n",
    "y_proba = pipeline.predict_proba(X_test)\n",
    "roc_auc = roc_auc_score(y_test, y_proba, multi_class=\"ovr\", average=\"macro\")\n",
    "print(f\"ROC AUC Score (Macro Average): {roc_auc:.2f}\")\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_test, y_pred, average=\"macro\")\n",
    "print(f\"Precision (Macro Average): {precision:.2f}\")\n",
    "\n",
    "# Recall\n",
    "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "print(f\"Recall (Macro): {recall_macro:.2f}\")\n",
    "\n",
    "# F1\n",
    "f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4413fe23-4801-4aaa-b05d-c18fc882d5b7",
   "metadata": {},
   "source": [
    "## Building Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab260d4-9b2c-4495-9c09-12bd60d031a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "pipeline1 = imbpipeline([\n",
    "    ('preprocessor', preprocessor), \n",
    "    ('smote' , SMOTE(random_state = 12345)),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipeline1.fit(X_train, y_train)\n",
    "y_pred = pipeline1.predict(X_test)\n",
    "\n",
    "###  Evaluation metrics\n",
    "\n",
    "# Accuracy \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# ROC AUC\n",
    "y_proba = pipeline1.predict_proba(X_test)\n",
    "roc_auc = roc_auc_score(y_test, y_proba, multi_class=\"ovr\", average=\"macro\")\n",
    "print(f\"ROC AUC Score (Macro Average): {roc_auc:.2f}\")\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_test, y_pred, average=\"macro\")\n",
    "print(f\"Precision (Macro Average): {precision:.2f}\")\n",
    "\n",
    "# Recall\n",
    "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "print(f\"Recall (Macro): {recall_macro:.2f}\")\n",
    "\n",
    "# F1\n",
    "f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a350be2b-7be2-43af-8633-26b63d58eccd",
   "metadata": {},
   "source": [
    "## Building Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf7a4dc-2212-4887-8ee4-c44bb623f363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Perceptron\n",
    "\n",
    "pipeline1 = imbpipeline([ \n",
    "    ('preprocessor', preprocessor), \n",
    "    ('smote' , SMOTE(random_state = 12345)),\n",
    "    ('classifier', Perceptron())\n",
    "])\n",
    "\n",
    "pipeline1.fit(X_train, y_train)\n",
    "y_pred = pipeline1.predict(X_test)\n",
    "\n",
    "###  Evaluation metrics\n",
    "\n",
    "# Accuracy \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_test, y_pred, average=\"macro\")\n",
    "print(f\"Precision (Macro Average): {precision:.2f}\")\n",
    "\n",
    "# Recall\n",
    "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "print(f\"Recall (Macro): {recall_macro:.2f}\")\n",
    "\n",
    "# F13\n",
    "f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44be56dd-ce66-4067-bb53-e67b913e78b9",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22ce0af-8132-44f1-900e-7d7d1f8aa816",
   "metadata": {},
   "source": [
    "Logistic Regression performed the best out of the three models after SMOTE was used to alleviate the class imbalance. In order to increase performance in the minority classes, the model was able to modify the decision threshold and better handle the synthetic samples produced by SMOTE since it could produce probability estimates for each class.\n",
    "\n",
    "Even with the advantage of the synthetic samples, KNN was still unable to overcome the imbalance because of its dependence on nearest neighbors, which can cause bias in favor of the majority class, particularly in high-dimensional data.\n",
    "\n",
    "Since the perceptron is a linear model, it only slightly improved with SMOTE, producing less-than-ideal results because it was unable to accurately represent the intricate interactions between the classes.\n",
    "\n",
    "In conclusion, once SMOTE was used, the most appropriate model for this multi-class classification problem was logistic regression due to its adaptability and capacity to account for class imbalance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
